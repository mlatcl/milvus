{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d155f22-e150-4c34-9a57-9c7167d45a65",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Mapping\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e02a4-d686-4f33-b621-0ba8a92afd66",
   "metadata": {},
   "source": [
    "# A minimalist system description\n",
    "\n",
    "Starting off from [our experiments](https://github.com/mlatcl/kafker) with Kafka and [Faust](https://faust.readthedocs.io/en/latest/), I wondered how a Python \"DSL\" for system descriptions that heavily hints towards data oriented architectures might look like. On a high level, I'd like to:\n",
    "\n",
    "- A system is built from streams that contain state and mostly stateless computational nodes\n",
    "- We do not only want to capture the \"real world\" system but also the assumptions we make about this system, such as mdoels we want to employ or knowledge that we will need.\n",
    "- All components in the system need to play nice: No state that is not in a stream!\n",
    "- Nodes should be black boxes to some degree: We don't really care what's going on inside them and calculations might be really complex. However, the results of computations that are relevant to the system should be exposed to the outside.\n",
    "\n",
    "Ideally, a complete system description would contain\n",
    "\n",
    "- The context/environment in which calculations take place\n",
    "- Deterministic computations that carry semantics but are not ML\n",
    "- Modelling assumptions such as inputs/outputs, model choices, inference methods, ...\n",
    "- Model state such as artifacts/training results\n",
    "- The \"production\" environment, i.e. monitoring tools, metrics, ...\n",
    "\n",
    "I'd like to make a distinction between _(computational) infrastructure choices_ which I would not see as part of the system description (it does not really matter which learning framework is used), and _domain choices_ which are what we would like to focus on (what form does the data have, where does it come from, which parameters are relevant, ...). We'd like to capture as much knowledge as possible and avoid imposing computational choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c7e51-d75b-4525-bcfc-d17b1ebe4a15",
   "metadata": {},
   "source": [
    "# A small system\n",
    "\n",
    "I'm mostly following Faust's async design here, because it is very succinct an pretty close to how I'd like a DSL to look like. This DSL is represented by a dummy library called `milvus`. The following system is pretty much the simplest use-case I could come up with: We're ingesting sensor data - temperature and fan rpm readings - from some machine. The data is slightly preprocessed and published to some stream. The machine has a number of properties that will be used to analyze the data.\n",
    "\n",
    "This part introduces the main data structures, with one important difference from Faust:\n",
    "\n",
    "- **Record**: A record is a description of the data contained in a stream, pretty much a data class.\n",
    "- **Stream**: A stream (topic) is a potentially infinite queue that contains the _data_ that flows through the system.\n",
    "- **Node**: A (computational) node reads data from input streams and publishes data to output streams.\n",
    "- **Context**: A context is a special kind of stream: It contains meta-information about the system that describs the system and might change, but it is not data generated by the system.\n",
    "\n",
    "I introduced a distinction between streams and contexts here because I think these kinds of information need to be handled differently on a semantic level. Both things could probably be mapped to a Kafka topic, but a context is not something we would do machine learning on, we would use it to inform our machine learning tasks. A stream would contain data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "846adcad-5043-40e9-b605-335d956d8182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sensors(milvus.Record):\n",
    "    timestamp: datetime\n",
    "    temperature: float\n",
    "    fan_rpm: float\n",
    "\n",
    "\n",
    "sensor_api = milvus.data_sources.REST(\"sensor_api\", poll=True)\n",
    "sensor_readings = milvus.stream(\"sensor_readings\", value_type=Sensors)\n",
    "\n",
    "\n",
    "@milvus.node(inputs=[sensor_api], outputs=[sensor_readings])\n",
    "async def ingest_sensor_data(self, sensor_events):\n",
    "    async for event in sensor_events:\n",
    "        yield Sensors(\n",
    "            **event,\n",
    "            timestamp=milvus.parse_time(event[\"strange_timetamp\"]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a9eefb-00bf-45f0-961d-5e5dfe81235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine(milvus.Record):\n",
    "    name: str\n",
    "    fan_control_curve: Mapping[float, float]\n",
    "    alert_temperature: float\n",
    "\n",
    "\n",
    "machine_properties = milvus.context(\"machine_properties\", value_type=Machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf49ca-27b2-40cb-860f-c99470b132d9",
   "metadata": {},
   "source": [
    "# Adding a predictive model\n",
    "The code above fully describes a simple real-world system. We now add machine learning to the mix.\n",
    "The building blocks above map nicely to the ML context.\n",
    "Inputs and outputs are streams and snapshots of streams could nicely be captured in more classical ML datasets.\n",
    "The configuration of a model (hyperparameters) and atrifacts produced by the model (weights) ar emore context than stream:\n",
    "We could deploy a model to production with exacly one set of weights, but changing the weights changes the system and a well-formualted data oriented architecture could do this on the fly.\n",
    "\n",
    "We can map concepts from ML libraries such as Keras to this DSL:\n",
    "A computational node could be interpreted as a step in data preprocessing, yielding a `Sequential` unit of steps that generate intermediate data.\n",
    "A `Model` could just be a computational node with well-defined inputs and outputs.\n",
    "\n",
    "We could also identify a single layer of - say - a deep GP with a model and connect layers via streams.\n",
    "While this is nice on a conceptional level, it will be quite challenging to do efficient inference in that setting.\n",
    "Note however, that if we want to propagate uncertainties between models, the lines will probably blur somewhat here:\n",
    "We'll have to do \"joint\" inference in differnt nodes of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed8108bc-0245-420d-8bac-9417321ad20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_temperatures = milvus.stream(\"predicted_temperatures\", value_type=float)\n",
    "\n",
    "model_config = milvus.context(\"model_config\", value_type=dict)\n",
    "model_artifacts = milvus.context(\"model_artifacts\")\n",
    "\n",
    "preprocessing_pipeline = milvus.Sequential(\n",
    "    {\n",
    "        \"add_history\": milvus.Windowed(minutes=10),\n",
    "        \"clean_data\": milvus.Filter(\n",
    "            lambda window: len(window) > 10 and not np.any(np.isnan(window))\n",
    "        ),\n",
    "        \"whiten\": milvus.Whiten(),\n",
    "    }\n",
    ")\n",
    "\n",
    "is_next_temperature_bad = milvus.Model(\n",
    "    \"is_next_temperature_bad\",\n",
    "    model=milvus.models.MagicTemperatureClassificationModel,\n",
    "    inputs=preprocessing_pipeline(sensor_readings),\n",
    "    outputs=predicted_temperatures,\n",
    "    config=model_config,\n",
    "    artifacts=model_artifacts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a91398-4d79-41a2-ab11-5f6266bd4377",
   "metadata": {},
   "source": [
    "# Monitoring\n",
    "Besides cleanly formulating ML models, we can use the graphy nature of the system description to formulate monitoring tasks.\n",
    "This example introduces a new context to configure problematic events and then plugs into the system in the right places.\n",
    "We do not need to change things above to get to what we need - pretty cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "324028b5-4cc0-425a-82e8-b14632915483",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "system_alerts = milvus.stream(\"system_alerts\", value_type=str)\n",
    "rpm_monitoring_config = milvus.context(\n",
    "    \"rpm_monitoring_config\",\n",
    "    default={\n",
    "        \"acceptable_deviation_ratio\": 0.2,\n",
    "        \"acceptable_deviations_per_window\": 2,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "@milvus.node(\n",
    "    inputs=[preprocessing_pipeline[\"clean_data\"]],\n",
    "    contexts=[machine_properties, rpm_monitoring_config],\n",
    "    outputs=[system_alerts],\n",
    ")\n",
    "async def check_rpm(windows):\n",
    "    def is_rpm_bad(sensor_state):\n",
    "        rpm_deviation = abs(\n",
    "            sensor_state.rpm\n",
    "            - machine_properties.fan_control_curve(sensor_state.temperature)\n",
    "        )\n",
    "        return (\n",
    "            rpm_deviation / sensor_state.rpm\n",
    "            > rpm_monitoring_config[\"acceptable_deviation_ratio\"]\n",
    "        )\n",
    "\n",
    "    async for window in windows:\n",
    "        num_deviations = sum(is_rpm_bad(sensor_state) for sensor_state in window)\n",
    "        if num_deviations > rpm_monitoring_config[\"acceptable_deviations_per_window\"]:\n",
    "            yield \"Too many deviations!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc58156-2ecc-4a83-9a05-8c23ea3fe38d",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "## Technical Level\n",
    "The system above can be visualized as a graph containing three types of nodes:\n",
    "\n",
    "- **Green Streams** that contain data\n",
    "- **Yellow Nodes** that perform calculation\n",
    "- **Violet Contexts** that specify the interface to the outside world\n",
    "\n",
    "These different types of system information are typically handled independently:\n",
    "\n",
    "- Tools like Kafka care about data and don't know about computation\n",
    "- Tools like Tensorflow/Pytorch care about computation and don't know about data\n",
    "- Context is injected through application code and only an implicit part of the system description or design\n",
    "\n",
    "I wonder: Is AutoAI about adding Context to the mix to empower us to dynamically reason about data and computation?\n",
    "\n",
    "## Organization Level\n",
    "The example above can be thought of as being implemented in independent teams A, B and C.\n",
    "The technical system information can form an interface for communication between the teams.\n",
    "This interfaces could be implemented in a highly-coupled fashion through shared code or through an information broker in a larger organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2adcc03-6756-4243-bd88-82dd7defacd6",
   "metadata": {},
   "source": [
    "![temperature system](images/temperature_system.png)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
